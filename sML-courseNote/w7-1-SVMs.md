# Terminology
-   `Support Vector Machine`
    In machine learning, support vector machines (SVMs) are supervised learning models that represent examples as points in space and try to find a (linear) decision boundary so that examples of different categories are divided by as wide of a gap as possible.
    SVMs can use what is called the kernel trick to learn the right way to "project" examples as points in space in order to find good decision boundaries. SVMs are an example of large-margin classifiers.
    [Link to Wikipedia article](https://en.wikipedia.org/wiki/Support_vector_machine)

-   `Regression`
    Regression (or regression analysis) is a statistical process for estimating the relationships among variables. Typically, there is an output or dependent variable (such as the price of a house) and one or more predictors or features (such as the area of a house or the number of bedrooms). Specifically, regression models estimate how the "typical" value of the output variable changes when one of the features is changed (but the others are fixed constant). A common example of regression analysis is linear regression where the dependent variable is modeled as a linear function of the features.
    [Link to Wikipedia article](https://en.wikipedia.org/wiki/Regression_analysis)


-   `Decision Boundary`
    A decision boundary is a curve or surface that divides the underlying space into two sets, one for each class.
    The classifier classifies all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Decision_boundary)

-   `Large Margin Classifier`
    A large margin classifier (or just a margin classifier) is a type of classification model that defines the distance of an example from the decision boundary and attempts to find a decision boundary that not just separates examples of different classes but also maximizes the distance of examples to the boundary (in the appropriate direction).
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Margin_classifier)

-   `Regularization`
    In regression or classification, regularization refers to  the process of adding additional terms to our cost function, often to introduce a preference for simpler models. Regularization is often used as a technique to prevent overfitting.

-   `Gaussian Kernel`
    The Gaussian function refers to a function  of the form $f(x) = a.e^{-\frac{(x-b)^2}{2c^2}}$, where $a$, $b$, and $c$ are parameters of the function.
    The probability distribution of a 1-dimensional Guassian distribution is a Gaussian function.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Gaussian_function)

-   `Kernel Function`
    A kernel function is a function that takes two inputs and returns a number representing how similar they are.
    Intuitively, we can think of kernel functions a way of defining some notion of similarity between examples.
    Kernel functions have the property that they are positive-definite functions.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Positive-definite_kernel)

-   `Linear Function`
    A linear function is a function where the output is a polynomial of degree zero or one. For example, a linear function with *1* variable $f(x)$ has the form $a \time x + b$
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Linear_function)


-   `Similarity Function`
    A similarity function is a function that maps two inputs to a number representing how similar they are. **Kernel functions** are a *popular* choice for similarity functions.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Similarity_measure)


-   `Polynomial`
    A polynomial is a math expression that consist of variables that involve the addition, subtraction or product of such variables.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Polynomial)

-   `Derivative`
    The derivative of a function measures the sensitivity of the function value with respect to a change in its argument (input value).
    Intuitively, the derivative tells us how much the function value will change if we change the input values slightly.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Derivative)

-   `Feature Vector`
    A feature vector is a vector of numbers that represent properties of an object that we want to use in our prediction or learning task.
    For example, if we want to predict the price of a house using its area and the number of rooms it contains, we would use a 2-dimensional feature vector to represent the house where one dimension would be the are of the house and the second dimension that number of rooms.
    [Link to wikipedia article](https://en.wikipedia.org/wiki/Feature_vector)

-   `Parameter Vector`
    The parameter vector of a model refers to a vector that capture some **relationship** between the `input` variables (features) and the `output` of a model.


# Large Margin Machines



# Kernels



# SVMs in Practice
